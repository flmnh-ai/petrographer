---
title: "Petrography Model Training"
format: html
editor: visual
---

**Training workflow** for petrographic detection models using Detectron2. This notebook demonstrates both local and HPC training approaches with seamless R-Python integration.

**Key Features:**

-   Unified interface for local and HPC training
-   Automatic data synchronization and job management\
-   Real-time progress monitoring
-   Seamless model deployment after training

## Setup and Dependencies

```{r setup}
library(magick)
library(reticulate) 
library(tidyverse)
library(future)

# Setup Python dependencies
py_require(c('sahi', 'torch', 'torchvision', 'opencv-python', 'scikit-image', 'pandas'))
py_require('detectron2@git+https://github.com/facebookresearch/detectron2.git@4fa166c043ca45359cd7080523b7122e7e0f9d91')
sahi <- import('sahi')
skimage <- import('skimage')
# Load training functions
source('R/petrography_simple.R')
```

## Configuration

Set up paths and training parameters.

```{r config}
# Data configuration
DATA_DIR <- "data/processed/background_removal"  # Directory with train/ and val/ subdirectories
OUTPUT_NAME <- "background_removal_v2"
NUM_CLASSES <- 2

# Training parameters
MAX_ITER <- 2000
LEARNING_RATE <- 0.00025
DEVICE <- "cuda"  # Options: "cpu", "cuda", "mps"
EVAL_PERIOD <- 100  # Validation evaluation frequency
CHECKPOINT_PERIOD <- 999  # 0 = only save final model (gets converted to 999999 internally)

# HPC configuration (set to NULL for local training)
HPC_HOST <- "hpg"  # Change to "hpg" for HPC training
HPC_USER <- "nicolas.gauthier"

cat("📋 Training Configuration:\n")
cat("- Data directory:", DATA_DIR, "\n")
cat("- Model name:", OUTPUT_NAME, "\n") 
cat("- Max iterations:", MAX_ITER, "\n")
cat("- Device:", DEVICE, "\n")
cat("- Eval period:", EVAL_PERIOD, "iterations\n")
cat("- Checkpoint period:", if(CHECKPOINT_PERIOD == 0) "final only" else paste(CHECKPOINT_PERIOD, "iterations"), "\n")
cat("- Training mode:", if(is.null(HPC_HOST)) "Local" else paste("HPC (", HPC_HOST, ")", sep=""), "\n")
```

## Data Validation

Verify training data structure and annotations.

```{r data-validation}
# Check data directory structure
train_dir <- file.path(DATA_DIR, "train")
val_dir <- file.path(DATA_DIR, "val")

cat("📁 Data Structure Validation:\n")
cat("- Train directory exists:", dir.exists(train_dir), "\n")
cat("- Val directory exists:", dir.exists(val_dir), "\n")

if (dir.exists(train_dir)) {
  train_images <- list.files(train_dir, pattern = "\\.(jpg|jpeg|png)$", ignore.case = TRUE)
  train_annotations <- file.exists(file.path(train_dir, "_annotations.coco.json"))
  cat("- Train images:", length(train_images), "\n")
  cat("- Train annotations:", train_annotations, "\n")
}

if (dir.exists(val_dir)) {
  val_images <- list.files(val_dir, pattern = "\\.(jpg|jpeg|png)$", ignore.case = TRUE)
  val_annotations <- file.exists(file.path(val_dir, "_annotations.coco.json"))
  cat("- Val images:", length(val_images), "\n")
  cat("- Val annotations:", val_annotations, "\n")
}

# Calculate dataset size for sync planning
if (dir.exists(DATA_DIR)) {
  dataset_size <- sum(file.info(list.files(DATA_DIR, recursive = TRUE, full.names = TRUE))$size, na.rm = TRUE)
  dataset_size_mb <- round(dataset_size / 1024^2, 1)
  cat("- Total dataset size:", dataset_size_mb, "MB\n")
}
```

## Python Environment Check

Verify the Python environment that will be used for training.

```{r python-env}
# Display Python configuration
py_config <- py_config()

cat("🐍 Python Environment:\n")
cat("- Python executable:", py_config$python, "\n")
cat("- Python version:", as.character(py_config$version), "\n")
cat("- Virtual environment:", py_config$virtualenv, "\n")

# Check key dependencies
cat("\n📦 Key Dependencies:\n")
dependencies <- c("torch", "detectron2", "sahi")

for (dep in dependencies) {
  available <- tryCatch({
    import(dep)
    TRUE
  }, error = function(e) FALSE)
  
  cat("- ", dep, ":", if(available) "✅ Available" else "❌ Missing", "\n")
}
```

## Training Execution

Run the training with monitoring and progress updates.

```{r training}
# Set up future plan for async operations (if using HPC)
if (!is.null(HPC_HOST)) {
  plan(multisession)
  cat("🔄 Configured async processing for HPC monitoring\n")
}

# Record start time
start_time <- Sys.time()
cat("🚀 Starting training at:", format(start_time), "\n\n")

# Execute training
model_path <- train_model(
  data_dir = DATA_DIR,
  output_name = OUTPUT_NAME,
  max_iter = MAX_ITER,
  num_classes = NUM_CLASSES,
  device = DEVICE,
  eval_period = EVAL_PERIOD,
  checkpoint_period = CHECKPOINT_PERIOD,
  hpc_host = HPC_HOST,
  hpc_user = HPC_USER,
  hpc_base_dir = '/blue/nicolas.gauthier/nicolas.gauthier'  # Sync only if not using HPC
)

# Record completion time
end_time <- Sys.time()
duration <- as.numeric(difftime(end_time, start_time, units = "mins"))

cat("\n🎉 Training completed!\n")
cat("- Duration:", round(duration, 1), "minutes\n")
cat("- Model path:", model_path, "\n")
```

## Post-Training Evaluation

Load and evaluate the newly trained model.

```{r evaluation}
# Evaluate training metrics
eval_result <- evaluate_training(model_dir = model_path)

cat("📊 Training Evaluation:\n")
cat("- Output directory:", eval_result$output_dir, "\n")
cat("- Training metrics available:", eval_result$summary$metrics_available, "\n")
cat("- Validation metrics available:", eval_result$summary$validation_metrics_available, "\n")
cat("- Total iterations:", eval_result$summary$total_iterations, "\n")
cat("- Validation evaluations:", eval_result$summary$validation_evaluations, "\n")

if (nrow(eval_result$training_data) > 0) {
  final_metrics <- tail(eval_result$training_data, 1)
  cat("- Final total loss:", round(final_metrics$total_loss, 4), "\n")
  cat("- Final learning rate:", format(final_metrics$lr, scientific = TRUE), "\n")
}
```

## Training Progress Visualization

Plot training curves to assess model convergence.

```{r training-plots, fig.width=12, fig.height=10}
if (nrow(eval_result$training_data) > 0) {
  training_df <- eval_result$training_data
  
  # Training Loss curves
  loss_cols <- training_df %>% 
    select(contains("loss")) %>% 
    names()
  
  if (length(loss_cols) > 0) {
    p_loss <- training_df %>%
      select(iteration, all_of(loss_cols)) %>%
      pivot_longer(cols = -iteration, names_to = "loss_type", values_to = "loss_value") %>%
      filter(!is.na(loss_value)) %>%
      ggplot(aes(x = iteration, y = loss_value, color = loss_type)) +
      geom_line(alpha = 0.8) +
      facet_wrap(~loss_type, scales = "free_y") +
      labs(title = paste("Training Loss Curves -", OUTPUT_NAME), 
           x = "Iteration", y = "Loss") +
      theme_minimal() +
      theme(legend.position = "none")
    
    print(p_loss)
  }
  
  # Validation metrics plots
  if (nrow(eval_result$validation_data) > 0) {
    validation_df <- eval_result$validation_data
    
    # Get validation metric columns (typically bbox/AP metrics)
    val_cols <- validation_df %>% 
      select(-iteration) %>% 
      select_if(~ !all(is.na(.))) %>%
      names()
    
    if (length(val_cols) > 0) {
      p_validation <- validation_df %>%
        select(iteration, all_of(val_cols)) %>%
        pivot_longer(cols = -iteration, names_to = "metric_type", values_to = "metric_value") %>%
        filter(!is.na(metric_value)) %>%
        ggplot(aes(x = iteration, y = metric_value, color = metric_type)) +
        geom_line(alpha = 0.8) + 
        geom_point(alpha = 0.6, size = 1) +
        facet_wrap(~metric_type, scales = "free_y") +
        labs(title = paste("Validation Metrics -", OUTPUT_NAME),
             x = "Iteration", y = "Metric Value") +
        theme_minimal() +
        theme(legend.position = "none")
      
      print(p_validation)
    }
  }
  
  # Learning rate schedule
  if ("lr" %in% names(training_df)) {
    p_lr <- training_df %>%
      filter(!is.na(lr)) %>%
      ggplot(aes(x = iteration, y = lr)) +
      geom_line(color = "blue", alpha = 0.8) +
      scale_y_log10() +
      labs(title = paste("Learning Rate Schedule -", OUTPUT_NAME), 
           x = "Iteration", y = "Learning Rate (log scale)") +
      theme_minimal()
    
    print(p_lr)
  }
} else {
  cat("No training metrics available for visualization\n")
}
```

## Model Testing

Test the newly trained model on a sample image.

```{r model-testing}
# Load the new model
new_model <- load_model(
  model_path = file.path(model_path, "model_final.pth"),
  config_path = file.path(model_path, "config.yaml"),
  confidence = 0.5,
  device = DEVICE
)

cat("✅ Successfully loaded new model\n")

# Test on a sample image
sample_images <- list.files(file.path(DATA_DIR, "val"), 
                           pattern = "\\.(jpg|jpeg|png)$", 
                           full.names = TRUE, ignore.case = TRUE)

if (length(sample_images) > 0) {
  test_image <- sample_images[1]
  cat("🔬 Testing on:", basename(test_image), "\n")
  
  # Run prediction
  test_result <- predict_image(
    image_path = test_image,
    model = new_model,
    use_slicing = TRUE,
    slice_size = 1500,
    output_dir = file.path("results", paste0(OUTPUT_NAME, "_test"))
  )
  
  cat("📋 Test Results:\n")
  cat("- Objects detected:", nrow(test_result), "\n")
  
  if (nrow(test_result) > 0) {
    cat("- Mean confidence:", round(mean(test_result$confidence), 3), "\n")
    cat("- Mean area:", round(mean(test_result$area)), "pixels\n")
  }
} else {
  cat("No validation images found for testing\n")
}
```

Plot predicted image

```{r}
list.files(file.path("results", paste0(OUTPUT_NAME, "_test")), full.names = TRUE)
predicted_image <- image_read("results/background_removal_v2_test/8AL458-1-2-7_jpeg.rf.12b96c51460b5d822788d93c3fd20121_prediction.png")
  
plot(predicted_image)
```

## Training Summary

Summarize the training session and next steps.

```{r summary}
cat("📋 Training Session Summary\n")
cat("Model Name:", OUTPUT_NAME, "\n")
cat("Training Duration:", round(duration, 1), "minutes\n")
cat("Training Mode:", if(is.null(HPC_HOST)) "Local" else paste("HPC (", HPC_HOST, ")"), "\n")
cat("Model Location:", model_path, "\n")
cat("Total Iterations:", eval_result$summary$total_iterations, "\n")

if (exists("test_result") && nrow(test_result) > 0) {
  cat("Test Detection Count:", nrow(test_result), "\n")
}

cat("\n📚 Next Steps:\n")
cat("1. Review training curves for convergence\n")
cat("2. Test model on additional validation images\n") 
cat("3. Compare performance with previous models\n")
cat("4. Deploy model for production use\n")

# Reset future plan
if (!is.null(HPC_HOST)) {
  plan(sequential)
}
```
