---
title: "Petrographic Image Analysis with Detectron2 and SAHI"
format: html
params:
  single_image: "data/raw/Lyons_data/c/139-45_xp_000019.jpg"
  batch_input_dir: "data/raw/Lyons_data/c"
  output_dir: !r here::here("results")
  use_slicing: TRUE
  slice_size: 512
  overlap_ratio: 0.2
  confidence: 0.5
  device: "cpu"
---

**Main analysis workflow** for petrographic thin section images using Detectron2 with SAHI. This notebook provides a complete pipeline from model evaluation to morphological analysis with clean R-Python integration.

**Key Features:**

-   Python-based ML with Detectron2 + SAHI for detection
-   R-based analysis and visualization with tidyverse
-   Clean interface through modular R functions
-   Comprehensive morphological property analysis

## Setup

```{r setup, message=FALSE}
library(magick)
library(tidyverse)
library(scico)
library(patchwork)
library(glue)
library(fs)
library(here)
library(petrographer)

# Configuration
config <- list(
  single_image = params$single_image,
  batch_input_dir = params$batch_input_dir,
  output_dir = params$output_dir,
  use_slicing = params$use_slicing,
  slice_size = params$slice_size,
  overlap_ratio = params$overlap_ratio,
  confidence = params$confidence,
  device = params$device
)

# Create output directory
dir_create(config$output_dir)

cat("\n=== Analysis Configuration ===\n")
for(name in names(config)) {
  if(name != "batch_input_dir") {
    cat(sprintf("%s: %s\n", name, config[[name]]))
  }
}
```

## Analysis Functions

We now use the optimized functions from `petrography_simple.R` which separate model loading from prediction for maximum efficiency. The model is loaded once and reused across all predictions.

```{r show-functions}
cat("\n== Available Analysis Functions ==\n")
cat("• load_model() - Load detection model once for reuse\n")
cat("• predict_image() - Analyze single image with loaded model\n")
cat("• predict_images() - Process multiple images efficiently\n")
cat("• evaluate_training() - Assess model performance\n")
cat("• enhance_results() - Add derived metrics\n")
cat("• summarize_by_image() - Per-image statistics\n")
cat("• get_population_stats() - Overall population metrics\n")
```

## Model Loading

Load the trained detection model once for efficient reuse across all predictions.

```{r load-model}
cat("\n== Loading Model ==\n")

# Load the detection model with optimized settings
model <- load_model(
  confidence = config$confidence,
  device = config$device
)

cat(sprintf("Model path: %s\n", path_file(model$model_path)))
cat(sprintf("Config path: %s\n", path_file(model$config_path)))
cat(sprintf("Confidence threshold: %s\n", model$confidence))
cat(sprintf("Device: %s\n", model$device))
```

## Model Training Evaluation

Evaluate the performance of our trained model by analyzing training logs.

```{r training-evaluation}
cat("\n== Training Evaluation ==\n")

# Run training evaluation
eval_result <- evaluate_training()

cat(sprintf("Output directory: %s\n", eval_result$output_dir))
cat(sprintf("Metrics available: %s\n", eval_result$summary$metrics_available))
cat(sprintf("Training data records: %d\n", nrow(eval_result$training_data)))
cat(sprintf("Total iterations: %s\n", eval_result$summary$total_iterations))

# Final metrics from training data
if (nrow(eval_result$training_data) > 0) {
  training_df <- eval_result$training_data
  final_loss <- tail(training_df$total_loss[!is.na(training_df$total_loss)], 1)
  final_lr <- tail(training_df$lr[!is.na(training_df$lr)], 1)
  
  cat("\n=== Final Training Metrics ===\n")
  cat(sprintf("Final total loss: %.4f\n", final_loss))
  cat(sprintf("Final learning rate: %s\n", format(final_lr, scientific = TRUE)))
}
```

Display training curves and evaluation metrics.

```{r training-plots, fig.width=12, fig.height=8}
# Training loss curves
training_df <- eval_result$training_data
loss_cols <- training_df %>% 
  select(contains("loss")) %>% 
  names()

p_loss <- training_df %>%
  select(iteration, all_of(loss_cols)) %>%
  pivot_longer(cols = -iteration, names_to = "loss_type", values_to = "loss_value") %>%
  filter(!is.na(loss_value)) %>%
  ggplot(aes(x = iteration, y = loss_value, color = loss_type)) +
  geom_line() +
  facet_wrap(~loss_type, scales = "free_y") +
  labs(title = "Training Loss Curves", x = "Iteration", y = "Loss") +
  theme_minimal() +
  theme(legend.position = "none")

print(p_loss)

# Learning rate schedule
p_lr <- training_df %>%
  filter(!is.na(lr)) %>%
  ggplot(aes(x = iteration, y = lr)) +
  geom_line(color = "blue") +
  scale_y_log10() +
  labs(title = "Learning Rate Schedule", x = "Iteration", y = "Learning Rate (log)") +
  theme_minimal()

print(p_lr)
```

## Single Image Analysis

Demonstrate detailed analysis on a single image.

```{r single-image-analysis}
cat("\n== Single Image Analysis ==\n")

# Run prediction on single image using loaded model
single_result <- predict_image(
  image_path = config$single_image,
  model = model,
  use_slicing = config$use_slicing,
  slice_size = config$slice_size,
  overlap = config$overlap_ratio,
  output_dir = path(config$output_dir, "single_image")
)

cat(sprintf("Image analyzed: %s\n", path_file(config$single_image)))
cat(sprintf("Objects detected: %d\n", nrow(single_result)))
```

```{r single-image-display, fig.width=30, fig.height=12}
# Original image
original_img <- image_read(config$single_image)

# Prediction image
pred_files <- dir_ls(path(config$output_dir, "single_image"), 
                    regexp = "*_prediction.png")

if (length(pred_files) > 0) {
  pred_img <- image_read(pred_files[1])
  
  # Display side by side
  combined <- image_append(c(original_img, pred_img))
  par(mar = c(0, 0, 0, 0))
  plot(combined)
} else {
  cat("Warning: No prediction images found\n")
}
```

Analyze morphological properties of detected objects.

```{r single-image-morphology}
cat("\n=== Morphological Analysis ===\n")

if (nrow(single_result) > 0) {
  # Display summary statistics
  morph_stats <- get_population_stats(single_result)
  
  cat(sprintf("Objects detected: %d\n", nrow(single_result)))
  cat(sprintf("Total area: %d pixels\n", round(morph_stats$total_area)))
  cat(sprintf("Mean area: %d pixels\n", round(morph_stats$mean_area)))
  cat(sprintf("Mean circularity: %.3f\n", morph_stats$mean_circularity))
  cat(sprintf("Mean eccentricity: %.3f\n", morph_stats$mean_eccentricity))
} else {
  cat("No objects detected for morphological analysis\n")
}
  
# Interactive data table
single_result %>%
  mutate(across(where(is.numeric), ~round(.x, 1)))
```

Create morphological analysis plots.

```{r single-morphology-plots, fig.width=12, fig.height=10}
if (nrow(single_result) > 0) {
  # Size distribution
  p1 <- single_result %>%
    ggplot(aes(x = area)) +
    geom_histogram(bins = 20, fill = "steelblue", alpha = 0.7) +
    scale_x_log10() +
    labs(title = "Particle Size Distribution", x = "Area (log scale)", y = "Count") +
    theme_minimal()

  # Eccentricity distribution
  p2 <- single_result %>%
    ggplot(aes(x = eccentricity)) +
    geom_histogram(bins = 20, fill = "coral", alpha = 0.7) +
    labs(title = "Eccentricity Distribution", x = "Eccentricity", y = "Count") +
    theme_minimal()

  # Orientation distribution
  p3 <- single_result %>%
    ggplot(aes(x = orientation_deg)) +
    geom_histogram(bins = 20, fill = "forestgreen", alpha = 0.7) +
    labs(title = "Orientation Distribution", x = "Orientation (degrees)", y = "Count") +
    theme_minimal()

  # Circularity vs Area
  p4 <- single_result %>%
    ggplot(aes(x = area, y = circularity)) +
    geom_point(alpha = 0.6) +
    scale_x_log10() +
    labs(title = "Circularity vs Size", x = "Area (log scale)", y = "Circularity") +
    theme_minimal()

  # Combine plots
  (p1 + p2) / (p3 + p4)
} else {
  cat("No data available for morphological plots\n")
}
```

## Batch Processing

Process multiple images and analyze population statistics.

```{r batch-processing}
cat("\n== Batch Processing ==\n")

# Create a temp directory with a few images for demo
temp_batch_dir <- path(config$output_dir, "temp_batch_input")
dir_create(temp_batch_dir)

# Find and copy first few images for demo
all_images <- dir_ls(config$batch_input_dir, 
                    regexp = "(?i)\\.(jpg|jpeg|png|tiff|tif)$")

# Copy first 5 images to temp directory
demo_images <- head(all_images, 5)
for (img in demo_images) {
  file_copy(img, temp_batch_dir)
}

cat(sprintf("Copied %d images for batch demo\n", length(demo_images)))

# Run batch prediction using loaded model
batch_result <- predict_images(
  input_dir = temp_batch_dir,
  model = model,
  use_slicing = config$use_slicing,
  slice_size = config$slice_size,
  overlap = config$overlap_ratio,
  output_dir = path(config$output_dir, "batch_results"),
  save_visualizations = TRUE
)

cat("\n=== Batch Processing Results ===\n")
cat(sprintf("Images processed: %d\n", length(unique(batch_result$image_name))))
cat(sprintf("Total objects detected: %d\n", nrow(batch_result)))
cat(sprintf("Output directory: %s\n", path(config$output_dir, "batch_results")))
```

Analyze batch results and population statistics.

```{r batch-analysis}
cat("\n=== Population Analysis ===\n")

if (nrow(batch_result) > 0) {
  # Population statistics
  pop_stats <- get_population_stats(batch_result)
  
  cat(sprintf("Total images processed: %d\n", pop_stats$unique_images))
  cat(sprintf("Total objects detected: %d\n", pop_stats$total_objects))
  cat(sprintf("Average objects per image: %.1f\n", pop_stats$mean_objects_per_image))
  cat(sprintf("Total area detected: %d pixels\n", round(pop_stats$total_area)))
  cat(sprintf("Mean object area: %d pixels\n", round(pop_stats$mean_area)))

  # Summary statistics by image using package function
  batch_summary <- summarize_by_image(batch_result)
  
  # Display summary table
  batch_summary %>%
    mutate(across(where(is.numeric), ~round(.x, 3)))
} else {
  cat("No batch data available for analysis\n")
}
```

Create population-level visualizations.

```{r batch-visualizations, fig.width=14, fig.height=12}
if (nrow(batch_result) > 0) {
  # Population size distribution
  p1 <- batch_result %>%
    ggplot(aes(x = area)) +
    geom_histogram(bins = 30, fill = "steelblue", alpha = 0.7) +
    scale_x_log10() +
    labs(title = "Population Size Distribution", 
         x = "Area (log scale)", y = "Count") +
    theme_minimal()

  # Size distribution by image
  p2 <- batch_result %>%
    ggplot(aes(x = area, fill = image_name)) +
    geom_density(alpha = 0.5) +
    scale_x_log10() +
    labs(title = "Size Distribution by Image", 
         x = "Area (log scale)", y = "Density") +
    theme_minimal() +
    theme(legend.position = "none")

  # Morphological properties correlation
  p3 <- batch_result %>%
    slice_sample(n = min(500, nrow(batch_result))) %>%  # Sample for performance
    ggplot(aes(x = eccentricity, y = circularity, color = log10(area))) +
    geom_point(alpha = 0.6) +
    scale_color_viridis_c() +
    labs(title = "Morphological Properties", 
         x = "Eccentricity", y = "Circularity", 
         color = "Log Area") +
    theme_minimal()

  # Box plots by image
  p4 <- batch_result %>%
    ggplot(aes(x = image_name, y = area)) +
    geom_boxplot(alpha = 0.7) +
    scale_y_log10() +
    labs(title = "Size Distribution by Image", 
         x = "Image", y = "Area (log scale)") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

  # Combine plots
  (p1 + p2) / (p3 + p4)
} else {
  cat("No batch data available for visualization\n")
}
```

## Summary

Complete analysis workflow using modern R and Python integration for petrographic image analysis.
