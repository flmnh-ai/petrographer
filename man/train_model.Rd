% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/training.R
\name{train_model}
\alias{train_model}
\title{Train a new petrography detection model}
\usage{
train_model(
  data_dir,
  output_name,
  num_classes,
  max_iter = 12000,
  learning_rate = 5e-04,
  device = "cuda",
  eval_period = 1000,
  checkpoint_period = 0,
  ims_per_batch = NA,
  auto_scale_lr = TRUE,
  num_workers = NULL,
  hpc_env = NULL,
  hpc_cpus_per_task = NULL,
  hpc_mem = NULL,
  gpus = 1,
  hpc_host = Sys.getenv("PETROGRAPHER_HPC_HOST", ""),
  hpc_user = NULL,
  hpc_base_dir = Sys.getenv("PETROGRAPHER_HPC_BASE_DIR", ""),
  local_output_dir = here::here("Detectron2_Models"),
  rsync_mode = c("update", "mirror"),
  publish_after_train = FALSE,
  model_board = NULL,
  model_description = NULL
)
}
\arguments{
\item{data_dir}{Directory containing \verb{train/} and \verb{valid/} subdirectories with COCO annotations.}

\item{output_name}{Name for the trained model (used for artifact directories and pin names).}

\item{max_iter}{Maximum training iterations. Default: 12000.}

\item{learning_rate}{Base learning rate before auto-scaling by batch (default: 5e-4).
Effective LR = learning_rate * (ims_per_batch / 16) when \code{auto_scale_lr = TRUE}.}

\item{device}{Device for local training: 'cpu', 'cuda', or 'mps' (default: 'cuda').}

\item{eval_period}{Validation evaluation frequency in iterations (default: 500).}

\item{checkpoint_period}{Checkpoint saving frequency (0 = final only; > 0 = every N iters).}

\item{ims_per_batch}{Total images per iteration across all GPUs. If NA (default), uses 2 images per GPU.}

\item{auto_scale_lr}{If TRUE (default), scale LR linearly by global batch vs. reference batch 16.}

\item{num_workers}{DataLoader workers per process (Detectron2). If NULL (default), set to images per GPU.}

\item{hpc_env}{Character vector of SLURM script preamble lines (e.g., module loads). If NULL, none added.}

\item{hpc_cpus_per_task}{Optional SLURM cpus-per-task hint.}

\item{hpc_mem}{Optional SLURM memory hint.}

\item{gpus}{Number of GPUs for HPC training (default: 1; ignored for local).}

\item{hpc_host}{SSH hostname for HPC training (default: \code{PETROGRAPHER_HPC_HOST}; empty for local).}

\item{hpc_user}{Username for HPC (default: NULL).}

\item{hpc_base_dir}{Remote base directory on HPC (default: \code{PETROGRAPHER_HPC_BASE_DIR}).}

\item{local_output_dir}{Local directory to save trained model (default: \code{Detectron2_Models}).}

\item{rsync_mode}{Data sync mode: 'update' (default) or 'mirror' (adds --delete).}

\item{publish_after_train}{Whether to publish (pin) the trained model to a board.}

\item{model_board}{pins board for model storage (if NULL and \code{publish_after_train=TRUE}, uses \code{\link[=pg_board]{pg_board()}}).}

\item{model_description}{Optional description to include with the published model.}
}
\value{
Path to the trained model directory.
}
\description{
Orchestrates local or HPC training using Detectron2. R computes batch size,
workers, and a batch-scaled learning rate, then calls the Python trainer.
Optionally publishes the resulting model to a pins board.
}
